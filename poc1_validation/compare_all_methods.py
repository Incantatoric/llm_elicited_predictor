#!/usr/bin/env python3
"""
Comprehensive comparison of all three methods:
1. Bayesian with LLM-elicited priors
2. Bayesian with uninformed priors  
3. Naive LLM (in-context learning)

This implements the complete PoC 1 validation from the Capstick paper.
"""

from bayesian_evaluator import BayesianEvaluator
from naive_llm_evaluator import NaiveLLMEvaluator
import pandas as pd
import numpy as np
import sys
import os
from pathlib import Path


def run_all_evaluations():
    """Run all three evaluation methods"""
    
    print("üöÄ COMPREHENSIVE EVALUATION: PoC 1 VALIDATION")
    print("=" * 80)
    print("Implementing Capstick et al. methodology for ÌïúÌôîÏÜîÎ£®ÏÖò prediction")
    print("=" * 80)
    
    # Check API key setup
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ùå OPENAI_API_KEY environment variable not set!")
        print("Please set it with: export OPENAI_API_KEY='your-key-here'")
        print("Or copy config/env.example to .env and fill in your key")
        return None, None, None
    
    results = {}
    
    # 1. Bayesian with LLM-elicited priors
    print(f"\n{'üß† METHOD 1: BAYESIAN + LLM-ELICITED PRIORS'}")
    print("-" * 60)
    try:
        evaluator1 = BayesianEvaluator(prior_type="elicited")
        results['elicited'] = evaluator1.run_evaluation()
        print("‚úÖ Elicited Bayesian completed")
    except Exception as e:
        print(f"‚ùå Elicited Bayesian failed: {e}")
        results['elicited'] = None
    
    # 2. Bayesian with uninformed priors
    print(f"\n{'üìä METHOD 2: BAYESIAN + UNINFORMED PRIORS'}")
    print("-" * 60)
    try:
        evaluator2 = BayesianEvaluator(prior_type="uninformed")
        results['uninformed'] = evaluator2.run_evaluation()
        print("‚úÖ Uninformed Bayesian completed")
    except Exception as e:
        print(f"‚ùå Uninformed Bayesian failed: {e}")
        results['uninformed'] = None
    
    # 3. Naive LLM (in-context learning)
    print(f"\n{'ü§ñ METHOD 3: NAIVE LLM (IN-CONTEXT LEARNING)'}")
    print("-" * 60)
    try:
        evaluator3 = NaiveLLMEvaluator()
        results['naive_llm'] = evaluator3.run_evaluation()
        print("‚úÖ Naive LLM completed")
    except Exception as e:
        print(f"‚ùå Naive LLM failed: {e}")
        results['naive_llm'] = None
    
    return results


def create_comparison_table(results):
    """Create comprehensive comparison table"""
    
    print(f"\n{'üìã COMPREHENSIVE COMPARISON TABLE'}")
    print("=" * 90)
    
    # Prepare data for comparison
    methods = []
    metrics_data = []
    
    method_names = {
        'elicited': 'Bayesian + LLM Priors',
        'uninformed': 'Bayesian + Uninformed',
        'naive_llm': 'Naive LLM (In-Context)'
    }
    
    for method, result in results.items():
        if result is not None:
            methods.append(method_names[method])
            metrics_data.append(result)
    
    if len(metrics_data) == 0:
        print("‚ùå No successful results to compare")
        return None
    
    # Create comparison DataFrame
    comparison_data = {}
    
    # Standard metrics
    standard_metrics = ['mae', 'rmse', 'r_squared', 'directional_accuracy']
    
    for metric in standard_metrics:
        comparison_data[metric] = []
        for data in metrics_data:
            if metric in data:
                comparison_data[metric].append(data[metric])
            else:
                comparison_data[metric].append(np.nan)
    
    # Coverage metrics (may not exist for all methods)
    coverage_metrics = ['coverage_95', 'coverage_90']
    for metric in coverage_metrics:
        if any(metric in data for data in metrics_data):
            comparison_data[metric] = []
            for data in metrics_data:
                comparison_data[metric].append(data.get(metric, np.nan))
    
    # Create DataFrame
    df = pd.DataFrame(comparison_data, index=methods)
    
    # Print formatted table
    print(f"{'Method':<25} {'MAE':<8} {'RMSE':<8} {'R¬≤':<8} {'Dir.Acc':<10} {'Cov.95%':<8} {'Cov.90%':<8}")
    print("-" * 90)
    
    for i, method in enumerate(methods):
        mae = df.iloc[i]['mae'] if not np.isnan(df.iloc[i]['mae']) else 'N/A'
        rmse = df.iloc[i]['rmse'] if not np.isnan(df.iloc[i]['rmse']) else 'N/A'
        r2 = df.iloc[i]['r_squared'] if not np.isnan(df.iloc[i]['r_squared']) else 'N/A'
        dir_acc = df.iloc[i]['directional_accuracy'] if not np.isnan(df.iloc[i]['directional_accuracy']) else 'N/A'
        cov95 = df.iloc[i].get('coverage_95', np.nan)
        cov90 = df.iloc[i].get('coverage_90', np.nan)
        
        # Format values
        mae_str = f"{mae:.4f}" if mae != 'N/A' else 'N/A'
        rmse_str = f"{rmse:.4f}" if rmse != 'N/A' else 'N/A' 
        r2_str = f"{r2:.4f}" if r2 != 'N/A' else 'N/A'
        dir_str = f"{dir_acc:.2%}" if dir_acc != 'N/A' else 'N/A'
        cov95_str = f"{cov95:.2%}" if not np.isnan(cov95) else 'N/A'
        cov90_str = f"{cov90:.2%}" if not np.isnan(cov90) else 'N/A'
        
        print(f"{method:<25} {mae_str:<8} {rmse_str:<8} {r2_str:<8} {dir_str:<10} {cov95_str:<8} {cov90_str:<8}")
    
    # Save detailed comparison
    results_dir = Path("data/results/comprehensive_comparison")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # Save CSV
    df.to_csv(results_dir / "method_comparison.csv")
    
    # Save detailed results
    import json
    with open(results_dir / "detailed_results.json", 'w') as f:
        # Convert numpy types to native Python types for JSON serialization
        serializable_results = {}
        for method, result in results.items():
            if result is not None:
                serializable_results[method] = {
                    k: float(v) if isinstance(v, (np.float32, np.float64)) else 
                       int(v) if isinstance(v, (np.int32, np.int64)) else v
                    for k, v in result.items()
                }
            else:
                serializable_results[method] = None
        
        json.dump(serializable_results, f, indent=2)
    
    print(f"\n‚úÖ Detailed comparison saved to: {results_dir}")
    
    return df


def analyze_results(results, comparison_df):
    """Analyze and interpret results according to Capstick paper"""
    
    print(f"\n{'üìä RESULTS ANALYSIS (Capstick et al. Framework)'}")
    print("=" * 70)
    
    if comparison_df is None or len(comparison_df) < 2:
        print("‚ùå Insufficient results for analysis")
        return
    
    # Key findings based on paper expectations
    findings = []
    
    # Check if LLM-elicited priors outperform uninformed
    if 'elicited' in results and 'uninformed' in results:
        if results['elicited'] and results['uninformed']:
            elicited_mae = results['elicited']['mae']
            uninformed_mae = results['uninformed']['mae']
            
            if elicited_mae < uninformed_mae:
                improvement = (uninformed_mae - elicited_mae) / uninformed_mae * 100
                findings.append(f"‚úÖ LLM-elicited priors reduce MAE by {improvement:.1f}% vs uninformed")
            else:
                degradation = (elicited_mae - uninformed_mae) / uninformed_mae * 100
                findings.append(f"‚ö†Ô∏è LLM-elicited priors increase MAE by {degradation:.1f}% vs uninformed")
    
    # Check directional accuracy improvements
    if 'elicited' in results and results['elicited']:
        dir_acc = results['elicited']['directional_accuracy']
        findings.append(f"üìà LLM-elicited directional accuracy: {dir_acc:.1%}")
    
    # Compare with naive LLM approach
    if 'naive_llm' in results and 'elicited' in results:
        if results['naive_llm'] and results['elicited']:
            naive_mae = results['naive_llm']['mae']
            elicited_mae = results['elicited']['mae']
            
            if elicited_mae < naive_mae:
                improvement = (naive_mae - elicited_mae) / naive_mae * 100
                findings.append(f"‚úÖ Bayesian approach beats naive LLM by {improvement:.1f}% (MAE)")
            else:
                findings.append(f"‚ö†Ô∏è Naive LLM outperforms Bayesian approach")
    
    # Print findings
    for finding in findings:
        print(finding)
    
    # Paper-style conclusion
    print(f"\n{'üéØ CONCLUSION (Paper Validation)'}")
    print("-" * 40)
    
    if len(findings) == 0:
        print("‚ùå Insufficient data for conclusive analysis")
    else:
        best_method = comparison_df['mae'].idxmin() if 'mae' in comparison_df.columns else None
        if best_method:
            print(f"üèÜ Best performing method: {best_method}")
        
        # Check if results align with paper expectations
        if 'elicited' in results and results['elicited']:
            print("üìù LLM-elicited priors successfully implemented")
            print("üìä Bayesian mixture model with 100 components working")
            
            # Data efficiency analysis
            print(f"üí° This approach should be most beneficial in low-data regimes")
            print(f"   (Current test uses {len(comparison_df)} data points)")


def main():
    """Main execution function"""
    
    # Run all evaluations
    results = run_all_evaluations()
    
    if results is None:
        return 1
    
    # Create comparison
    comparison_df = create_comparison_table(results)
    
    # Analyze results
    analyze_results(results, comparison_df)
    
    print(f"\n{'üéâ PoC 1 VALIDATION COMPLETE'}")
    print("=" * 50)
    print("All three methods from Capstick et al. have been implemented and tested")
    print("Results saved to data/results/comprehensive_comparison/")
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 